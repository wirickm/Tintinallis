{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3c875893-dae5-47f1-8d0f-e7f09caf720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "import PyPDF2\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fdd767b8-24b7-403b-a98e-629667845c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "def extract_section(pdf_path, start_page, end_page):\n",
    "    \"\"\"\n",
    "    Extract text from a specific range of pages in a PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        start_page (int): Starting page number (1-based indexing).\n",
    "        end_page (int or None): Ending page number. If None, extracts till the end.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text.\n",
    "    \"\"\"\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        extracted_text = \"\"\n",
    "        \n",
    "        # Handle case where end_page is None (to end of document)\n",
    "        if end_page is None:\n",
    "            end_page = len(pdf_reader.pages)\n",
    "        \n",
    "        # Loop through the pages from start_page to end_page\n",
    "        for page_num in range(start_page - 1, end_page):  # PyPDF2 uses 0-based indexing\n",
    "            try:\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    extracted_text += page_text + \"\\n\"\n",
    "                else:\n",
    "                    print(f\"Warning: No text found on page {page_num + 1}\")\n",
    "            except IndexError:\n",
    "                print(f\"Error: Page number {page_num + 1} is out of range.\")\n",
    "                break\n",
    "        \n",
    "        return extracted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "83f84e20-5101-4326-9bfd-ff96b4b6570c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "def save_to_text(file_name, text):\n",
    "    \"\"\"\n",
    "    Save text to a file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the output text file.\n",
    "        text (str): Text content to save.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f573e96-5add-460b-8216-a3d18f6e4b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "def read_and_chunk_text(file_path, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Read text from a file and split it into chunks.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the text file.\n",
    "        chunk_size (int, optional): Size of each chunk in characters. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        list: List of text chunks.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    # Split text into chunks\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "21aa9e4e-f3b7-4b9b-b1d7-dc9fca62ada1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Original text.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Insert text-cleaning logic here (e.g., removing special characters, extra whitespace)\n",
    "    # For demonstration, we'll perform basic cleaning by stripping leading/trailing whitespace\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "839c3632-6b95-416a-a101-e439ea083aff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "def process_chunks(chunks):\n",
    "    \"\"\"\n",
    "    Process and clean each text chunk.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cleaned text chunks.\n",
    "    \"\"\"\n",
    "    cleaned_chunks = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "        cleaned_chunk = clean_text(chunk)\n",
    "        cleaned_chunks.append(cleaned_chunk)\n",
    "    return cleaned_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2dce1187-404d-4dcb-b729-011eb4f7b265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "def print_chunks(processed_chunks):\n",
    "    \"\"\"\n",
    "    Print the first 1000 characters of each processed chunk.\n",
    "\n",
    "    Args:\n",
    "        processed_chunks (list): List of cleaned text chunks.\n",
    "    \"\"\"\n",
    "    for index, chunk in enumerate(processed_chunks):\n",
    "        print(f\"--- Printing chunk {index + 1} ---\\n\")\n",
    "        print(chunk[:1000])  # Print first 1000 characters for brevity\n",
    "        print('\\n' + '--' * 50 + '\\n')\n",
    "#print_chunks(processed_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9ef8cda5-8af1-4e44-a716-fd1eb027d1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "def create_messages(chunk, system_prompt):\n",
    "    \"\"\"\n",
    "    Create message list for OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        chunk (str): Text chunk to summarize.\n",
    "        system_prompt (str): System prompt defining the assistant's behavior.\n",
    "\n",
    "    Returns:\n",
    "        list: List of messages.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": chunk},\n",
    "        {\"role\": \"assistant\", \"content\": \"Please retain relevant information from the chunk.\"},\n",
    "\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f05553c8-7621-48d9-bfd1-82b00c5433c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "def summarize_chunk(chunk, system_prompt, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Summarize a text chunk using OpenAI.\n",
    "\n",
    "    Args:\n",
    "        chunk (str): Text chunk to summarize.\n",
    "        system_prompt (str): System prompt for the assistant.\n",
    "        model (str, optional): OpenAI model to use. Defaults to \"gpt-4\".\n",
    "\n",
    "    Returns:\n",
    "        str: Summary of the chunk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=create_messages(chunk, system_prompt),\n",
    "            max_tokens=5000,  # Adjust as needed\n",
    "            temperature=0.2,  # Adjust for creativity\n",
    "            #stream=True,\n",
    "        )\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error during summarization: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "#def summarize(url):\n",
    "#    website = Website(url)\n",
    "#    response = openai.chat.completions.create(\n",
    "#        model = \"gpt-4o-mini\",\n",
    "#        messages = messages_for(website)\n",
    "#    )\n",
    "#    return response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a5a38d6a-4fb9-4a4a-9166-2ed60c17b0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "import time  # Ensure time module is imported\n",
    "\n",
    "def summarize_chunks(processed_chunks, system_prompt, model=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Summarize all processed text chunks while tracking time taken per chunk and total time.\n",
    "\n",
    "    Args:\n",
    "        processed_chunks (list): List of cleaned text chunks.\n",
    "        system_prompt (str): System prompt for the assistant.\n",
    "        model (str, optional): OpenAI model to use. Defaults to \"gpt-4\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - summaries (list): List of summaries for each chunk.\n",
    "            - chunk_times (list): List of time durations for each chunk.\n",
    "            - total_time (float): Total time taken to summarize all chunks.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "    chunk_times = []\n",
    "    total_start_time = time.perf_counter()  # Start total timer\n",
    "\n",
    "    for i, chunk in enumerate(processed_chunks):\n",
    "        print(f\"🔄 Summarizing chunk {i+1}/{len(processed_chunks)}...\")\n",
    "        start_time = time.perf_counter()  # Start timer for this chunk\n",
    "\n",
    "        summary = summarize_chunk(chunk, system_prompt, model)\n",
    "        summaries.append(summary)\n",
    "\n",
    "        end_time = time.perf_counter()  # End timer for this chunk\n",
    "        duration = end_time - start_time\n",
    "        chunk_times.append(duration)\n",
    "\n",
    "        print(f\"✅ Chunk {i+1} summarized in {duration:.2f} seconds.\\n\")\n",
    "\n",
    "    total_end_time = time.perf_counter()  # End total timer\n",
    "    total_duration = total_end_time - total_start_time\n",
    "\n",
    "    print(f\"🎉 Summarized {len(summaries)} chunks in {total_duration:.2f} seconds total.\")\n",
    "\n",
    "    return summaries, chunk_times, total_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5af7c7a4-3b80-41fc-aa22-bc59cecdeceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "def assemble_summaries(summaries):\n",
    "    \"\"\"\n",
    "    Assemble all summaries into a single text.\n",
    "\n",
    "    Args:\n",
    "        summaries (list): List of summary strings.\n",
    "\n",
    "    Returns:\n",
    "        str: Assembled summaries.\n",
    "    \"\"\"\n",
    "    return '\\n\\n'.join(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "91811ec8-b5f8-46b1-881e-de9ffd8508d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "def write_summaries(cleaned_summaries, output_file):\n",
    "    \"\"\"\n",
    "    Write assembled summaries to a file.\n",
    "\n",
    "    Args:\n",
    "        cleaned_summaries (str): Assembled summaries.\n",
    "        output_file (str): Name of the output file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(cleaned_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7ff4f105-5676-449b-874b-3b3d17542828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# In[14]:\n",
    "# Load environment variables from the specified .env file\n",
    "env_path = 'C:/Users/MichaelJWirickJr/keys.env'  # Update this path if necessary\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check if the API key is loaded correctly (optional for debugging)\n",
    "if openai.api_key:\n",
    "    print(\"✅ OpenAI API Key loaded successfully.\")\n",
    "else:\n",
    "    print(\"❌ Error: OpenAI API Key not found. Please check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d502ddad-9d44-4e4c-92eb-27c45c1a437b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "# Define the system prompt for summarization\n",
    "system_prompt = (\n",
    "    \"You are an assistant that analyzes the contents of an emergency medicine book \"\n",
    "    \"please do not summarize, instead, retain as much relevant text as possible, ignoring text that might be chapter or section related. \"\n",
    ")#    \"Respond in markdown.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bfd648c0-1d6c-4594-bc11-2d51f0a4c081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "# Path to the PDF file (Update the path to your local PDF file)\n",
    "pdf_path = r'C:/Users/MichaelJWirickJr/Tintinallis_Emergency_Medicine/Tintinallis_Emergency_Medicine.pdf'\n",
    "\n",
    "# Define the sections with their adjusted page ranges\n",
    "sections = {\n",
    "    'Section 1': (20, 74),\n",
    "    'Section 2': (74, 88),\n",
    "    'Section 3': (88, 130),\n",
    "    'Section 4': (130, 186),\n",
    "    'Section 5': (186, 208),\n",
    "    'Section 6': (208, 270),\n",
    "    'Section 7': (270, 298),\n",
    "    'Section 8': (298, 332),\n",
    "    'Section 9': (332, 438),\n",
    "    'Section 10': (438, 508),\n",
    "    'Section 11': (508, 596),\n",
    "    'Section 12': (596, 646),\n",
    "    'Section 13': (646, 664),\n",
    "    'Section 14': (664, 704),\n",
    "    'Section 15': (704, 758),\n",
    "    'Section 16': (758, 792),\n",
    "    'Section 17': (792, 806),\n",
    "    'Section 18': (806, 862),\n",
    "    'Section 19': (862, 896),\n",
    "    'Section 20': (896, 926),\n",
    "    'Section 21': (926, 938),\n",
    "    'Section 22': (938, 946),\n",
    "    'Index': (946, None)  # None means to go to the end of the document\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e49b464f-4649-4dd1-afe6-2bda8b448e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Section 1 with pages 20 to 74\n"
     ]
    }
   ],
   "source": [
    "# In[17]:\n",
    "# Example: Process a specific section\n",
    "section_name = 'Section 1'  # Change as needed\n",
    "start_page, end_page = sections[section_name]\n",
    "\n",
    "print(f\"Selected {section_name} with pages {start_page} to {end_page if end_page else 'end of document'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2924a075-f54d-416b-bb73-83ca6755bf9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted text saved to Section 1.txt\n"
     ]
    }
   ],
   "source": [
    "# In[18]:\n",
    "# Step 1: Extract text for the selected section\n",
    "extracted_text = extract_section(pdf_path, start_page, end_page)\n",
    "\n",
    "# Step 2: Save the extracted text to a plain text file\n",
    "txt_file_name = f\"{section_name}.txt\"\n",
    "save_to_text(txt_file_name, extracted_text)\n",
    "print(f\"✅ Extracted text saved to {txt_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e2f11997-8a2f-4b7a-9c19-78ba972cadb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text split into 26 chunks.\n"
     ]
    }
   ],
   "source": [
    "# In[19]:\n",
    "# Step 3: Read and chunk the extracted text file\n",
    "chunks = read_and_chunk_text(txt_file_name)\n",
    "print(f\"✅ Text split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f34c4bb-a071-49a1-a2d3-8f1b0820c60d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/26...\n",
      "Processing chunk 2/26...\n",
      "Processing chunk 3/26...\n",
      "Processing chunk 4/26...\n",
      "Processing chunk 5/26...\n",
      "Processing chunk 6/26...\n",
      "Processing chunk 7/26...\n",
      "Processing chunk 8/26...\n",
      "Processing chunk 9/26...\n",
      "Processing chunk 10/26...\n",
      "Processing chunk 11/26...\n",
      "Processing chunk 12/26...\n",
      "Processing chunk 13/26...\n",
      "Processing chunk 14/26...\n",
      "Processing chunk 15/26...\n",
      "Processing chunk 16/26...\n",
      "Processing chunk 17/26...\n",
      "Processing chunk 18/26...\n",
      "Processing chunk 19/26...\n",
      "Processing chunk 20/26...\n",
      "Processing chunk 21/26...\n",
      "Processing chunk 22/26...\n",
      "Processing chunk 23/26...\n",
      "Processing chunk 24/26...\n",
      "Processing chunk 25/26...\n",
      "Processing chunk 26/26...\n",
      "✅ Processed 26 chunks.\n"
     ]
    }
   ],
   "source": [
    "# In[20]:\n",
    "# Step 4: Process each chunk (e.g., cleaning)\n",
    "processed_chunks = process_chunks(chunks)\n",
    "print(f\"✅ Processed {len(processed_chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d741b84d-6528-4ac2-95e9-f4f11dc1e391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chunk 1: 5000\n",
      "Length of chunk 2: 5000\n",
      "Length of chunk 3: 4999\n",
      "Length of chunk 4: 5000\n",
      "Length of chunk 5: 5000\n",
      "Length of chunk 6: 4999\n",
      "Length of chunk 7: 5000\n",
      "Length of chunk 8: 5000\n",
      "Length of chunk 9: 5000\n",
      "Length of chunk 10: 5000\n",
      "Length of chunk 11: 5000\n",
      "Length of chunk 12: 5000\n",
      "Length of chunk 13: 4999\n",
      "Length of chunk 14: 5000\n",
      "Length of chunk 15: 4999\n",
      "Length of chunk 16: 4999\n",
      "Length of chunk 17: 5000\n",
      "Length of chunk 18: 5000\n",
      "Length of chunk 19: 5000\n",
      "Length of chunk 20: 5000\n",
      "Length of chunk 21: 5000\n",
      "Length of chunk 22: 5000\n",
      "Length of chunk 23: 5000\n",
      "Length of chunk 24: 5000\n",
      "Length of chunk 25: 5000\n",
      "Length of chunk 26: 3496\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Process each chunk (e.g., cleaning)\n",
    "#processed_chunks = process_chunks(chunks)\n",
    "#print(f\"✅ Processed {len(processed_chunks)} chunks.\")\n",
    "\n",
    "# Print the length of each processed chunk\n",
    "for i, chunk in enumerate(processed_chunks, start=1):\n",
    "    print(f\"Length of chunk {i}: {len(chunk)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ad86e-58ad-4d3c-94c8-623d9f804a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[21]:\n",
    "# Optional: Print chunks for verification (prints first 1000 characters of each chunk)\n",
    "print_chunks(processed_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a88ad-cbdc-45a0-84b2-dedc7b52a938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[22]:\n",
    "# Step 5: Summarize each chunk using OpenAI\n",
    "#summaries = summarize_chunks(processed_chunks, system_prompt)\n",
    "#print(f\"✅ Summarized {len(summaries)} chunks.\")\n",
    "\n",
    "# In[22]:\n",
    "# Step 5: Summarize each chunk using OpenAI with timing\n",
    "\n",
    "# Start the summarization process and capture timing information\n",
    "summaries, chunk_times, total_time = summarize_chunks(processed_chunks, system_prompt)\n",
    "\n",
    "print(f\"✅ Summarized {len(summaries)} chunks.\")\n",
    "\n",
    "# Optional: Display detailed timing information\n",
    "print(\"\\n--- Timing Details ---\\n\")\n",
    "for idx, duration in enumerate(chunk_times, start=1):\n",
    "    print(f\"Chunk {idx}: {duration:.2f} seconds\")\n",
    "print(f\"\\n🔔 Total summarization time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e87a0de7-830d-44bf-b8c2-265c577d26f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Assembled all summaries into a single document.\n"
     ]
    }
   ],
   "source": [
    "# In[23]:\n",
    "# Step 6: Assemble all summaries into a single text\n",
    "assembled_summaries = assemble_summaries(summaries)\n",
    "print(\"✅ Assembled all summaries into a single document.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6847c6bc-5ce9-49d6-8870-b45512518eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summaries saved to Section 1_summaries.md\n"
     ]
    }
   ],
   "source": [
    "# In[24]:\n",
    "# Step 7: Save the assembled summaries to a new output file\n",
    "output_file_name = f\"{section_name}_summaries.md\"\n",
    "write_summaries(assembled_summaries, output_file_name)\n",
    "print(f\"✅ Summaries saved to {output_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9869787-52c3-41b1-840c-1a5776981c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In[25]:\n",
    "# Optional: Print the assembled summaries\n",
    "print(\"\\n--- Assembled Summaries ---\\n\")\n",
    "print(assembled_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7f8e50a7-1318-435e-bdfb-5d5127d03155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary of the Test Chunk:\n",
      "Control of the airway is the single most important task for emergency resuscitation.\n",
      "\n",
      "The initial approach to airway management involves simultaneous assessment and management of airway patency and oxygenation and ventilation. \n",
      "\n",
      "1. Assess the patient’s color and respiratory rate; respiratory or cardiac arrest may indicate immediate intubation.\n",
      "2. Open the airway with the head tilt–chin lift maneuver (use jaw thrust if C-spine injury is suspected). If needed, bag the patient with a bag-valve-mask device that includes an O2 reservoir. A good seal depends on proper mask size. This technique may require an oral or nasal airway or two rescuers (one to seal the mask with two hands and the other to bag the patient).\n",
      "3. Provide continuous monitoring of vital signs, oxygen saturation, and end-tidal CO2 (if possible).\n",
      "4. Determine the need for invasive airway management techniques. Do not wait for arterial blood gas analyses if the initial assessment indicates the need for invasive airway management. If the patient does not require immediate airway or ventilation control, administer oxygen by facemask to ensure an O2 saturation of 95%. Do not remove oxygen to draw an arterial blood gas analysis unless deemed safe from the initial assessment.\n",
      "5. Preoxygenate all patients prior to intubation regardless of saturation. Assess airway difficulty before initiation of advanced airway techniques.\n",
      "\n",
      "Orotracheal intubation is the most common means used to ensure a patent airway, prevent aspiration, and provide oxygenation and ventilation. Rapid sequence intubation (RSI) should be used unless the patient’s condition makes it unnecessary (e.g., cardiac arrest) or it is contraindicated due to an anticipated difficult airway.\n",
      "\n",
      "1. Prepare equipment, personnel, and drugs before attempting intubation. Assess airway difficulty and anticipate required airway rescue. Assemble and place suction, bag-valve-mask, and rescue devices within easy reach. Personnel should be present at the bedside to pass equipment or bag the patient if required.\n",
      "2. Ensure adequate ventilation and oxygenation and monitoring while preparing equipment. Preoxygenate with a non–rebreather oxygen mask at maximal oxygen flow rates or with a bag-valve-mask if the patient is not ventilating adequately.\n",
      "3. Select blade type and size (usually a No. 3 or 4 curved blade or a No. 2 or 3 straight blade); test the blade light. Select the tube size (usually 7.5 to 8.0 mm in women, 8.0 to 8.5 mm in men) and test the balloon cuff. The use of a flexible stylet is recommended.\n",
      "4. Position the patient with the head extended and neck flexed, possibly with a rolled towel under the occiput. If C-spine injury is suspected, maintain the head and neck in a neutral position with an assistant performing inline stabilization.\n",
      "5. With the handle in the operator’s left hand, insert the blade to push the tongue to the patient’s left and slowly advance to the epiglottis. Suctioning may be required. It is not uncommon to go past the larynx into the esophagus. Gradually withdraw the blade to reveal the epiglottis. If the curved blade is used, slide the tip into the vallecula and lift (indirectly lifting the epiglottis); if a straight blade is used, lift the epiglottis directly. Lift along the axis of the laryngoscope handle. Avoid levering the blade on the teeth to prevent dental trauma.\n",
      "6. Once the vocal cords are visualized, gently pass the tube between the cords. Remove the tube, check for tube placement by ventilating and listening for bilateral breath sounds and absence of epigastric sounds. Inflate the cuff.\n",
      "7. If the cords are not visualized, manipulate the thyroid cartilage using backward, upward, and rightward pressure (the “burp” maneuver) to help bring the cords into view. If unsuccessful, reoxygenation may need to be performed with a bag-valve-mask device. Consider changing the blade, the tube size, or the position of the patient before further attempts. Consider using an intubating stylet (bougie). Three unsuccessful attempts define a failed airway, and other rescue techniques must be considered.\n",
      "8. Confirm placement objectively with an end-tidal CO2 detector, capnography, or in cardiac arrest, an esophageal detection device. Check tube length; the usual distance from the corner of the mouth to 2 cm above the carina is 23 cm in men and 21 cm in women.\n",
      "9. Secure the tube and verify placement with a portable radiograph.\n",
      "\n",
      "Immediate complications include unrecognized esophageal intubation or mainstem bronchus intubation.\n"
     ]
    }
   ],
   "source": [
    "def qc():\n",
    "    \"\"\"\n",
    "    this code block is for testing for one chunk.\n",
    "    \"\"\"\n",
    "    pass \n",
    "# In[22]:\n",
    "# Step 5: Summarize a single chunk using OpenAI\n",
    "\n",
    "# Select the first chunk for testing\n",
    "test_chunk = processed_chunks[0]  # Change the index if you want to test a different chunk\n",
    "\n",
    "# Summarize the selected chunk\n",
    "summary = summarize_chunk(test_chunk, system_prompt)\n",
    "\n",
    "# Display the summary\n",
    "print(\"✅ Summary of the Test Chunk:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401399e-435a-4902-a503-f4e2b213f93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549e0ec-5bb4-4eef-8500-9498f37a76fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
